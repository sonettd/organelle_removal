{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Taxonomy Construction Tutorial (QIIME2 Artifact API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will walk through the dowloading of the Silva, Greengenes, Metaxa2, and PhytoRef taxonomies, and create extended versions of the Silva and Greengenes databases by supplementing them with the Metaxa2 and PhytoRef taxonomy sequences. These supplemented databases can be used for more accurate annotation of mitochondria as described in Sonett et al. [preprint](https://www.biorxiv.org/content/10.1101/2021.02.23.431501v2) using the [mitochondria removal tutorial found here](mitochondria_removal_protocol.ipynb).\n",
    "\n",
    "While we noticed a substantial difference in mitochondrial annotations when supplementing with Metaxa2 sequences, we saw little difference in chloroplast annotations with the addition of Phytoref sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "\n",
    "It's assumed that QIIME2 is installed, and it's virtual environment activated. This tutorial was tested with qiime2-amplicon-2024.2 running on Windows 10 via WSL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activating your QIIME2 virtual environment\n",
    "NOTE: you have to activate your QIIME2 virtual environment *before* starting this notebook. That's awkward since you're already in the notebook. You may need to exit the notebook, activate QIIME2, then restart it. I usually do this by first reminding myself of which virtual environments I have available:\n",
    "\n",
    "`conda env list`\n",
    "\n",
    "Here's an example of the output in my case:\n",
    "`# conda environments:\n",
    "\n",
    "base                  *  /Users/zaneveld/opt/anaconda3\n",
    "qiime2-2020.8            /Users/zaneveld/opt/anaconda3/envs/qiime2-2020.8\n",
    "qiime2-2021.11           /Users/zaneveld/opt/anaconda3/envs/qiime2-2021.11\n",
    "qiime2-amplicon-2023.9     /Users/zaneveld/opt/anaconda3/envs/qiime2-amplicon-2023.9\n",
    "`\n",
    "\n",
    "If I wanted to activate the `qiime2-amplicon-2023.9` virtual environment, I would then use: \n",
    "\n",
    "`conda activate qiime2-amplicon-2023.9`\n",
    "\n",
    ". You can run this tutorial from a Jupyter notebook in Terminal on Mac, any BASH command line interface in Linux or the Windows Subsystem for Linux as long as QIIME2 is installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrangement of tutorial files\n",
    "\n",
    "It's  assumed that you've downloaded the zipped tutorial with `input`, `output` and `procedure` folders, and that this script is within the provided procedure folder (where it starts). \n",
    "\n",
    "Given all that, this tutorial will discuss how to use the the supplemented databases to remove mitochondria from your 16S datasets using QIIME2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Check that we can import QIIME2 objects\n",
    "\n",
    "Before we go further, let's double check that we can import objects from QIIME2 by importing the core `Artifact` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from qiime2 import Artifact\n",
    "    print('Good to go!')\n",
    "except ModuleNotFoundError:\n",
    "    raise ModuleNotFoundError('\\nQIIME2 is not installed, or you are not in the proper environment.\\nPlease stop the Jupyter server, install QIIME2 or activate the environment, and restart this notebook.') from None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download referance taxonomy files from the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the gzip, os, shutil, subprocess, and tarfile libraries (for file system commands) and urllib (for the download function). Set working and reference directories, define a download function, and create the reference directory if it does not already exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import tarfile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = abspath('../output')\n",
    "refs_dir = join(working_dir, 'taxonomy_references')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, local_filepath):\n",
    "    with urllib.request.urlopen(url) as response, open(local_filepath, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(refs_dir):\n",
    "    os.mkdir(refs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unpack the Silva 138 SSU, Greengenes 13_8, Metaxa2, and Phytoref files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file('https://www.arb-silva.de/fileadmin/silva_databases/release_138/Exports/SILVA_138_SSURef_NR99_tax_silva_trunc.fasta.gz',\n",
    "              os.path.join(refs_dir, 'silva_sequences.fasta.gz'))\n",
    "download_file('https://www.arb-silva.de/fileadmin/silva_databases/release_138/Exports/taxonomy/taxmap_slv_ssu_ref_138.txt.gz',\n",
    "              os.path.join(refs_dir, 'silva_taxonomy.txt.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(os.path.join(refs_dir, 'silva_sequences.fasta.gz'), 'rb') as zipped:\n",
    "    with open(os.path.join(refs_dir, 'silva_sequences.fasta'), 'wb') as unzipped:\n",
    "        shutil.copyfileobj(zipped, unzipped)\n",
    "with gzip.open(os.path.join(refs_dir, 'silva_taxonomy.txt.gz'), 'rb') as zipped:\n",
    "    with open(os.path.join(refs_dir, 'silva_taxonomy.txt'), 'wb') as unzipped:\n",
    "        shutil.copyfileobj(zipped, unzipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/Dylan/Documents/zaneveld/organelle_removal/Tutorial/qiime2_API_tutorial/output/taxonomy_references/greengenes_sequences.fasta'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_file('ftp://greengenes.microbio.me/greengenes_release/gg_13_5/gg_13_8_otus.tar.gz',\n",
    "              os.path.join(refs_dir, 'gg_13_8_otus.tar.gz'))\n",
    "with tarfile.open(os.path.join(refs_dir, 'gg_13_8_otus.tar.gz'), 'r:gz') as tar:\n",
    "    tar.extractall(refs_dir)\n",
    "shutil.copyfile(os.path.join(refs_dir, 'gg_13_8_otus', 'rep_set', '99_otus.fasta'), os.path.join(refs_dir, 'greengenes_sequences.fasta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file('https://microbiology.se/sw/Metaxa2_2.2.1.tar.gz',\n",
    "              os.path.join(refs_dir, 'Metaxa2_2.2.1.tar.gz'))\n",
    "with tarfile.open(os.path.join(refs_dir, 'Metaxa2_2.2.1.tar.gz'), 'r:gz') as tar:\n",
    "    tar.extractall(refs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metaxa files are contained within a BLAST database and need further extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(refs_dir, 'Metaxa2_2.2.1/metaxa2_db/SSU'))\n",
    "subprocess.run(['blastdbcmd', '-entry', 'all', '-db', 'blast', '-out', 'metaxa2.fasta'])\n",
    "shutil.copyfile(os.path.join(refs_dir, os.path.join('Metaxa2_2.2.1', 'metaxa2_db', 'SSU', 'metaxa2.fasta')),\n",
    "                os.path.join(refs_dir, 'metaxa2.fasta'))\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file('http://phytoref.sb-roscoff.fr/static/downloads/PhytoRef_with_taxonomy.fasta',\n",
    "              os.path.join(refs_dir, 'PhytoRef_with_taxonomy.fasta'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create supplemented Silva and Greengenes reference taxonomies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Biopython, grab the sequences and sequence information from the supplmetal databases and add them to new taxonomy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silva and Greengenes have slightly different taxonomy naming schemes. We'll place the mitochondria sequences in a made-up family called \"Mitochondria\" under the order Rickettsiales. Chloroplasts are also different between Silva and Greengenes, so in Silva they go in order \"Chloroplast\" under class Cyanobacteriia, and in Greengenes they'll be class \"Chloroplast\" under phylum Cyanobacteria. \n",
    "\n",
    "Any information from the fasta files will be the species annotation, with the hope that if something is a strong enough match that the species info will populate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "silva_mitochondria_prefix = 'd__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__Mitochondria; g__Mitochondria; s__'\n",
    "silva_chloroplast_prefix = 'd__Bacteria; p__Cyanobacteria; c__Cyanobacteriia; o__Chloroplast; f__Chloroplast; g__Chloroplast; s__'\n",
    "greengenes_mitochondria_prefix = 'k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria; g__Mitochondria; s__'\n",
    "greengenes_chloroplast_prefix = 'k__Bacteria; p__Cyanobacteria; c__Chloroplast; o__Chloroplast; f__Chloroplast; g__Chloroplast; s__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(refs_dir, 'silva_organelle_taxonomy.tsv'), 'w') as silva_taxonomy:\n",
    "    with open(os.path.join(refs_dir, 'gg_organelle_taxonomy.tsv'), 'w') as gg_taxonomy:\n",
    "        silva_taxonomy.write('Feature ID\\tTaxon\\n')\n",
    "        gg_taxonomy.write('Feature ID\\tTaxon\\n')\n",
    "        with open(os.path.join(refs_dir, 'organelle_sequences.fasta'), 'w') as organelle_seqs:\n",
    "            for i, entry in enumerate(SeqIO.parse(os.path.join(refs_dir, 'metaxa2.fasta'), 'fasta')):\n",
    "                if 'mitochondria' in entry.description or 'Mitochondria' in entry.description:\n",
    "                    organelle_seqs.write(f'>metaxa2_mitochondria_{i}\\n')\n",
    "                    organelle_seqs.write(str(entry.seq + '\\n'))\n",
    "                    specific_info = str(entry.description).split(';')[-1]\n",
    "                    silva_taxonomy.write(f'metaxa2_mitochondria_{i}\\t{silva_mitochondria_prefix}{specific_info}\\n')\n",
    "                    gg_taxonomy.write(f'metaxa2_mitochondria_{i}\\t{greengenes_mitochondria_prefix}{specific_info}\\n')\n",
    "            for i, entry in enumerate(SeqIO.parse(refs_dir + '/PhytoRef_with_taxonomy.fasta', 'fasta')):\n",
    "                if not 'XXXXXXXXXX' in entry.seq:   #ditch the weird sequence\n",
    "                    organelle_seqs.write(f'>phytoref_chloroplast_{i}\\n')\n",
    "                    organelle_seqs.write(str(entry.seq + '\\n'))\n",
    "                    specific_info = str(entry.description).split('|')[-1]\n",
    "                    silva_taxonomy.write(f'phytoref_chloroplast_{i}\\t{silva_chloroplast_prefix}{specific_info}\\n')\n",
    "                    gg_taxonomy.write(f'phytoref_chloroplast_{i}\\t{greengenes_chloroplast_prefix}{specific_info}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import the sequence files into QIIME2 in order to merge them. We'll also save the base versions as QZAs.\n",
    "\n",
    "Silva publishes RNA sequences so we will convert that to DNA sequences as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiime2.plugins.rescript.methods import reverse_transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/Dylan/Documents/zaneveld/organelle_removal/Tutorial/qiime2_API_tutorial/output/taxonomy_references/gg_sequences.qza'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "organelle_seqs = Artifact.import_data('FeatureData[Sequence]', os.path.join(refs_dir, 'organelle_sequences.fasta'))\n",
    "silva_seqs_rna = Artifact.import_data('FeatureData[RNASequence]', os.path.join(refs_dir, 'silva_sequences.fasta'))\n",
    "silva_seqs = reverse_transcribe(silva_seqs_rna)\n",
    "greengenes_seqs = Artifact.import_data('FeatureData[Sequence]', os.path.join(refs_dir, 'greengenes_sequences.fasta'))\n",
    "\n",
    "silva_seqs.dna_sequences.save(os.path.join(refs_dir, 'silva_sequences_full.qza'))\n",
    "greengenes_seqs.save(os.path.join(refs_dir, 'gg_sequences_full.qza'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiime2.plugins.feature_table.methods import merge_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dylan/miniconda3/envs/qiime2-2024.2/lib/python3.8/site-packages/q2_types/feature_data/_transformer.py:258: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for id_, seq in data.iteritems():\n",
      "/home/dylan/miniconda3/envs/qiime2-2024.2/lib/python3.8/site-packages/q2_types/feature_data/_transformer.py:258: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for id_, seq in data.iteritems():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/Dylan/Documents/zaneveld/organelle_removal/Tutorial/qiime2_API_tutorial/output/taxonomy_references/greengenes_extended_sequences_full.qza'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silva_extended_seqs = merge_seqs([organelle_seqs, silva_seqs.dna_sequences])\n",
    "greengenes_extended_seqs = merge_seqs([organelle_seqs, greengenes_seqs])\n",
    "\n",
    "silva_extended_seqs.merged_data.save(os.path.join(refs_dir, 'silva_extended_sequences_full.qza'))\n",
    "greengenes_extended_seqs.merged_data.save(os.path.join(refs_dir, 'greengenes_extended_sequences_full.qza'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll select our region of interest. We use the V4 region in this example (and in the paper) based on the EMP protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import glob\n",
    "import tempfile\n",
    "import pysam\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from collections import defaultdict\n",
    "from qiime2 import Artifact\n",
    "from qiime2.plugins.feature_classifier.methods import classify_consensus_vsearch, extract_reads\n",
    "from qiime2.metadata import Metadata\n",
    "from qiime2.plugins.feature_table.methods import filter_samples, merge, merge_seqs, merge_taxa, rarefy\n",
    "from qiime2.plugins.taxa.methods import filter_table\n",
    "from qiime2.plugins.feature_table.visualizers import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tanya/Work_files/organelle_removal/output/taxonomy_references/gg_extended_sequences.qza'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import, select V4 region, merge, save\n",
    "organelle_seqs = Artifact.import_data('FeatureData[Sequence]',\n",
    "                                      refs_dir + '/organelle_sequences.fasta')\n",
    "v4_organelle_seqs, = extract_reads(organelle_seqs, 'GTGYCAGCMGCCGCGGTAA',\n",
    "                                   'GGACTACNVGGGTWTCTAAT', n_jobs = 24,\n",
    "                                   read_orientation = 'forward')\n",
    "silva_extended_seqs, = merge_seqs([v4_organelle_seqs,\n",
    "                                   Artifact.load(refs_dir +\n",
    "                                                 '/silva_sequences.qza')])\n",
    "#save the sequence files for both extended files\n",
    "silva_extended_seqs.save(refs_dir + '/silva_extended_sequences.qza')\n",
    "gg_seqs = Artifact.import_data('FeatureData[Sequence]', refs_dir +\n",
    "                               '/gg_13_8_otus/rep_set/99_otus.fasta')\n",
    "v4_gg_seqs, = extract_reads(gg_seqs, 'GTGYCAGCMGCCGCGGTAA',\n",
    "                            'GGACTACNVGGGTWTCTAAT', n_jobs = 24,\n",
    "                            read_orientation = 'forward')\n",
    "v4_gg_seqs.save(refs_dir + '/gg_sequences.qza')\n",
    "gg_extended_seqs, = merge_seqs([organelle_seqs, gg_seqs])\n",
    "gg_extended_seqs.save(refs_dir + '/gg_extended_sequences.qza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tanya/Work_files/organelle_removal/output/taxonomy_references/gg_extended_taxonomy.qza'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the taxonomy files for both extended files\n",
    "silva_organelle_taxonomy = Artifact.import_data('FeatureData[Taxonomy]',\n",
    "                                                refs_dir +\n",
    "                                                '/silva_organelle_taxonomy.tsv')\n",
    "silva_extended_taxonomy, = merge_taxa([silva_organelle_taxonomy,\n",
    "                                       Artifact.load(refs_dir +\n",
    "                                                     '/silva_taxonomy.qza')])\n",
    "silva_extended_taxonomy.save(refs_dir + '/silva_extended_taxonomy.qza')\n",
    "gg_taxonomy = Artifact.import_data('FeatureData[Taxonomy]', refs_dir +\n",
    "                                   '/gg_13_8_otus/taxonomy/99_otu_taxonomy.txt',\n",
    "                                   'HeaderlessTSVTaxonomyFormat')\n",
    "gg_taxonomy.save(refs_dir + '/gg_taxonomy.qza')\n",
    "gg_organelle_taxonomy = Artifact.import_data('FeatureData[Taxonomy]',\n",
    "                                             refs_dir +\n",
    "                                             '/gg_organelle_taxonomy.tsv')\n",
    "gg_extended_taxonomy, = merge_taxa([gg_organelle_taxonomy, gg_taxonomy])\n",
    "gg_extended_taxonomy.save(refs_dir + '/gg_extended_taxonomy.qza')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify taxonomy with vsearch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the directories and import files for this part of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = abspath('..')\n",
    "\n",
    "#import the metadata and sequence files from your analysis\n",
    "metadata_path = working_dir + '/input/sample_metadata_live_vs_dead_combo.tsv'\n",
    "seqs_path = working_dir + '/input/rep_seqs_merged.qza'\n",
    "\n",
    "#import the taxonomy files created in the previously created step.\n",
    "taxonomy_reference_dir = working_dir + '/output/taxonomy_references/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that all files exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying that all needed starting data files exist.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying that all needed starting data files exist.\")\n",
    "for existing_file in [working_dir,metadata_path,seqs_path,taxonomy_reference_dir]:\n",
    "    if not os.path.exists(existing_file):\n",
    "        raise IOError(f\"Required file {existing_file} not found. Please ensure it is in that directory.\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use vsearch to annotate taxonomy. This will be done once for each of the refernce taxonomies created in the last section: Greengenes, Silva, Greengenes + MeTaxa2 + phytoref reference mitochondrial sequences, Silva + MeTaxa2 + phytoref reference mitocondrial sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This step can take a wile to run. It is recommended to run this step overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = ['gg','silva','gg_extended','silva_extended']\n",
    "metadata = Metadata.load(metadata_path)\n",
    "seqs = Artifact.load(seqs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2-archive-6ri4ri5f/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2-archive-5j06xdmi/9d299643-4922-44bc-8003-3d5185d76805/data/dna-sequences.fasta --threads 4 --output_no_hits --blast6out /tmp/tmpirqojtnn\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_linux_x86_64, 15.5GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2-archive-5j06xdmi/9d299643-4922-44bc-8003-3d5185d76805/data/dna-sequences.fasta 100%\n",
      "51405917 nt in 202865 seqs, min 81, max 1003, avg 253\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching query sequences: 0 of 464 (0.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2-archive-6ri4ri5f/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2-archive-mjgxjjvw/b41681fb-a4e7-4ef8-a23a-a26f1bcfd272/data/dna-sequences.fasta --threads 4 --output_no_hits --blast6out /tmp/tmpqgopdrng\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_linux_x86_64, 15.5GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2-archive-mjgxjjvw/b41681fb-a4e7-4ef8-a23a-a26f1bcfd272/data/dna-sequences.fasta 100%\n",
      "86453445 nt in 313734 seqs, min 54, max 2366, avg 276\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching query sequences: 127 of 464 (27.37%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2-archive-6ri4ri5f/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2-archive-ewumxl2w/327da406-e7db-4898-931f-df01072fcefd/data/dna-sequences.fasta --threads 4 --output_no_hits --blast6out /tmp/tmpr3s8ims5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_linux_x86_64, 15.5GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2-archive-ewumxl2w/327da406-e7db-4898-931f-df01072fcefd/data/dna-sequences.fasta 100%\n",
      "303450669 nt in 213319 seqs, min 46, max 5604, avg 1423\n",
      "minseqlength 32: 1 sequence discarded.\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching query sequences: 373 of 464 (80.39%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2-archive-6ri4ri5f/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2-archive-4cipr0id/5a4b411f-26bc-4a88-9a05-7499d740c944/data/dna-sequences.fasta --threads 4 --output_no_hits --blast6out /tmp/tmpbso9jlee\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_linux_x86_64, 15.5GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2-archive-4cipr0id/5a4b411f-26bc-4a88-9a05-7499d740c944/data/dna-sequences.fasta 100%\n",
      "88535911 nt in 322907 seqs, min 54, max 2366, avg 274\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching query sequences: 127 of 464 (27.37%)\n"
     ]
    }
   ],
   "source": [
    "vsearch_results = {}\n",
    "for reference in references:\n",
    "    reference_otu_path = taxonomy_reference_dir + f'{reference}_sequences.qza'\n",
    "    reference_taxonomy_path = taxonomy_reference_dir + f'{reference}_taxonomy.qza'\n",
    "    reads = Artifact.load(reference_otu_path)\n",
    "    taxonomy = Artifact.load(reference_taxonomy_path)\n",
    "    vsearch_results[reference] = classify_consensus_vsearch(seqs, reads, taxonomy, threads = 4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save each of the taxonomy annotations for your sample sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reference in vsearch_results:\n",
    "    classification_taxonomy, = vsearch_results[reference]\n",
    "    classification_taxonomy.save(working_dir + '/output/' + str(reference) + '_reference_taxonomy.qza')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of mitochondria from samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up needed files for removal of mitochondria from samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filepath = abspath(\"../output\")\n",
    "input_filepath = abspath(\"../input\")\n",
    "output_dir = abspath(\"../output/filtered_tables\")\n",
    "\n",
    "#These 3 files are specific to your study\n",
    "feature_table_dir = working_dir + '/input/feature_table_live_vs_dead.qza'\n",
    "mapping_file = metadata_path\n",
    "sequence_file = seqs_path\n",
    "\n",
    "#Load the taxonomy files created in the last section.\n",
    "taxonomy_files = {\"greengenes_reference\":\"../output/gg_reference_taxonomy.qza\",\\\n",
    "                  \"greengenes_extended\": \"../output/gg_extended_reference_taxonomy.qza\",\\\n",
    "                  \"silva_reference\": \"../output/silva_reference_taxonomy.qza\",\\\n",
    "                  \"silva_extended\": \"../output/silva_extended_reference_taxonomy.qza\"}\n",
    "\n",
    "required_files = [feature_table,mapping_file,sequence_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all files exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying that all needed starting data files exist.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying that all needed starting data files exist.\")\n",
    "for existing_files in required_files:\n",
    "    if not exists(existing_file):\n",
    "        raise IOError(f\"Required file {existing_file} not found. Please ensure it is in the directory.\")\n",
    "        \n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate filtered tables using the default taxonomies for Greengenes and Silva as well as those supplemented with metaxa2 mitochondrial 16S rRNA and phytoref chloroplasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data using the greengenes_reference taxonomy (../output/gg_reference_taxonomy.qza)\n",
      "Removing mitochondia from: <artifact: FeatureTable[Frequency] uuid: 97994d43-af84-4dcd-a4a5-316bcc00a472>\n",
      "Saving results to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_greengenes_reference_mws.qza\n",
      "Saving file summary to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_greengenes_reference_mws.qzv\n",
      "Done with processing greengenes_reference taxonomy annotations!\n",
      "\n",
      "\n",
      "Analyzing data using the greengenes_extended taxonomy (../output/gg_extended_reference_taxonomy.qza)\n",
      "Removing mitochondia from: <artifact: FeatureTable[Frequency] uuid: 97994d43-af84-4dcd-a4a5-316bcc00a472>\n",
      "Saving results to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_greengenes_extended_mws.qza\n",
      "Saving file summary to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_greengenes_extended_mws.qzv\n",
      "Done with processing greengenes_extended taxonomy annotations!\n",
      "\n",
      "\n",
      "Analyzing data using the silva_reference taxonomy (../output/silva_reference_taxonomy.qza)\n",
      "Removing mitochondia from: <artifact: FeatureTable[Frequency] uuid: 97994d43-af84-4dcd-a4a5-316bcc00a472>\n",
      "Saving results to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_silva_reference_mws.qza\n",
      "Saving file summary to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_silva_reference_mws.qzv\n",
      "Done with processing silva_reference taxonomy annotations!\n",
      "\n",
      "\n",
      "Analyzing data using the silva_extended taxonomy (../output/silva_extended_reference_taxonomy.qza)\n",
      "Removing mitochondia from: <artifact: FeatureTable[Frequency] uuid: 97994d43-af84-4dcd-a4a5-316bcc00a472>\n",
      "Saving results to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_silva_extended_mws.qza\n",
      "Saving file summary to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_silva_extended_mws.qzv\n",
      "Done with processing silva_extended taxonomy annotations!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_feature_tables_by_taxonomy = defaultdict(dict)\n",
    "\n",
    "feature_table = Artifact.load(feature_table_dir)\n",
    "\n",
    "for label, taxonomy_file in taxonomy_files.items():\n",
    "    print(f\"Analyzing data using the {label} taxonomy ({taxonomy_file})\")\n",
    "    taxonomy = Artifact.load(taxonomy_file)\n",
    "    print(f\"Removing mitochondia from: {feature_table}\")\n",
    "    #Qiime2 API does not return a single object, but a named Tuple struture with each output.\n",
    "    filter_table_results = filter_table(feature_table,taxonomy,exclude=\"mitochondria,chloroplast\",mode=\"contains\")\n",
    "    filter_table_results = filter_samples(filter_table_results.filtered_table,metadata=metadata)\n",
    "    filtered_table = filter_table_results.filtered_table\n",
    "    \n",
    "    #save the file\n",
    "    output_filename = f\"feature_table_filtered_{label}_mws.qza\"\n",
    "    output_filepath = join(output_dir,output_filename)\n",
    "    print(f\"Saving results to: {output_filepath}\")\n",
    "    filtered_table.save(output_filepath)\n",
    "    \n",
    "    #output a file summary\n",
    "    summary_visualization = summarize(filtered_table,sample_metadata=metadata)\n",
    "    vis = summary_visualization.visualization\n",
    "    output_filename = f\"feature_table_filtered_{label}_mws.qzv\"\n",
    "    output_filepath = join(output_dir,output_filename)\n",
    "    print(f\"Saving file summary to: {output_filepath}\")\n",
    "    \n",
    "    filtered_feature_tables_by_taxonomy[label]=filtered_table\n",
    "    print(f\"Done with processing {label} taxonomy annotations!\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rarefy tables to an even depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: 44d128c1-3928-45c1-812b-5b4951168af0> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n",
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: 198b8748-a810-412f-b31a-ec9bbdd5def8> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n",
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: c6323e2b-0a78-4e72-939e-08cc227a6085> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n",
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: 5bb410e1-75b4-41c1-8887-073b4db4ac1e> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n"
     ]
    }
   ],
   "source": [
    "#choose a rarefaction depth appropriate for your study.\n",
    "rarefaction_depth = 1000\n",
    "\n",
    "rarefied_feature_tables_by_taxonomy = defaultdict(dict)\n",
    "\n",
    "for label,filtered_feature_tables in filtered_feature_tables_by_taxonomy.items():\n",
    "    print(f\"Rarefying feature table {filtered_feature_tables} to {rarefaction_depth} sequences/sample.\")\n",
    "    rarefy_results = rarefy(table=filtered_feature_tables,sampling_depth=rarefaction_depth)\n",
    "    #get the rarefied table out of the NamedTuple of results\n",
    "    rarefied_filtered_table = rarefy_results.rarefied_table\n",
    "        \n",
    "    #save the resulting feature table\n",
    "    output_filename = f\"feature_table_{label}_{rarefaction_depth}.qza\"\n",
    "    output_filepath = join(output_dir,output_filename)\n",
    "    print(\"Saving results to:{output_filepath}\")\n",
    "    rarefied_filtered_table.save(output_filepath)\n",
    "        \n",
    "    #store the rarefied tables in a dict so they don't need to relode them\n",
    "    rarefied_feature_tables_by_taxonomy[label] = rarefied_filtered_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
