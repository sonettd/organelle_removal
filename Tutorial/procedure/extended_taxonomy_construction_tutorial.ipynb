{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Taxonomy Construction Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This protocol will dowload the Silva, Greengenes, Metaxa2, and PhytoRef taxonomys. Next Silva and Greengenes databases are supplemented with the Metaxa2 and PhytoRef taxonomy databases. These supplemented databases are finaly used to remove mitochondria from 16S datasets using Qiime2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import python libraries and Qiime2 plugins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import tarfile\n",
    "import tempfile\n",
    "import pysam\n",
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "from os.path import abspath,exists,join\n",
    "from collections import defaultdict\n",
    "from qiime2 import Artifact\n",
    "from qiime2.plugins.feature_classifier.methods import classify_consensus_vsearch, extract_reads\n",
    "from qiime2.metadata import Metadata\n",
    "from qiime2.plugins.feature_table.methods import filter_samples, merge, merge_seqs, merge_taxa, rarefy\n",
    "from qiime2.plugins.taxa.methods import filter_table\n",
    "from qiime2.plugins.feature_table.visualizers import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.path.abspath('../output')\n",
    "refs_dir = working_dir + '/taxonomy_references'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create supplemented Siva and Greengenes reference taxonomies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dowload referance taxonomy files from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, local_filepath):\n",
    "    with urllib.request.urlopen(url) as response, open(local_filepath, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(refs_dir):\n",
    "    os.mkdir(refs_dir)\n",
    "if not os.path.isfile(refs_dir + '/silva_sequences.qza'):\n",
    "    download_file('https://data.qiime2.org/2021.2/common/silva-138-99-seqs-515-806.qza', \n",
    "                  refs_dir + '/silva_sequences.qza')\n",
    "if not os.path.isfile(refs_dir + '/silva_taxonomy.qza'):\n",
    "    download_file('https://data.qiime2.org/2021.2/common/silva-138-99-tax-515-806.qza', \n",
    "                  refs_dir + '/silva_taxonomy.qza')\n",
    "if not os.path.isfile(refs_dir + '/gg_13_8_otus.tar.gz'):\n",
    "    download_file('ftp://greengenes.microbio.me/greengenes_release/gg_13_5/gg_13_8_otus.tar.gz',\n",
    "                  refs_dir + '/gg_13_8_otus.tar.gz')\n",
    "if not os.path.isfile(refs_dir + '/Metaxa2_2.2.1.tar.gz'):\n",
    "    download_file('https://microbiology.se/sw/Metaxa2_2.2.1.tar.gz',\n",
    "                  refs_dir + '/Metaxa2_2.2.1.tar.gz')\n",
    "if not os.path.isfile(refs_dir + '/PhytoRef_with_taxonomy.fasta'):\n",
    "    download_file('http://phytoref.sb-roscoff.fr/static/downloads/PhytoRef_with_taxonomy.fasta',\n",
    "                  refs_dir + '/PhytoRef_with_taxonomy.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(refs_dir + '/gg_13_8_otus.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall(refs_dir)\n",
    "with tarfile.open(refs_dir + '/Metaxa2_2.2.1.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall(refs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run BLAST on the MeTaxa2 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(refs_dir + '/Metaxa2_2.2.1/metaxa2_db/SSU')\n",
    "!blastdbcmd -entry all -db blast -out metaxa2.fasta\n",
    "shutil.copyfile(refs_dir + '/Metaxa2_2.2.1/metaxa2_db/SSU/metaxa2.fasta', \n",
    "                refs_dir + '/metaxa2.fasta')\n",
    "os.chdir(refs_dir + '/metaxa2_extra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplement Silva and Greengenes taxonomy with MeTaxa2 and phytoref mitochondria and chloroplast sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(refs_dir + '/silva_organelle_taxonomy.tsv', 'w') as silva_taxonomy:\n",
    "    with open(refs_dir + '/gg_organelle_taxonomy.tsv', 'w') as gg_taxonomy:\n",
    "        silva_taxonomy.write('Feature ID\\tTaxon\\n')\n",
    "        gg_taxonomy.write('Feature ID\\tTaxon\\n')\n",
    "        with open(refs_dir + '/organelle_sequences.fasta', 'w') as organelle_seqs:\n",
    "            for i, entry in enumerate(SeqIO.parse(refs_dir + '/metaxa2.fasta', 'fasta')):\n",
    "                if 'mitochondria' in entry.description or 'Mitochondria' in entry.description:\n",
    "                    organelle_seqs.write('>metaxa2_mitochondria_' + str(i) + '\\n')\n",
    "                    organelle_seqs.write(str(entry.seq + '\\n'))\n",
    "                    specific_info = str(entry.description).split(';')[-1]\n",
    "                    silva_taxonomy.write('metaxa2_mitochondria_' + str(i) + '\\td__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__Mitochondria; g__Mitochondria; s__' + specific_info + '\\n')\n",
    "                    gg_taxonomy.write('metaxa2_mitochondria_' + str(i) + '\\tk__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria; g__Mitochondria; s__' + specific_info + '\\n')\n",
    "            for i, entry in enumerate(SeqIO.parse(refs_dir + '/PhytoRef_with_taxonomy.fasta', 'fasta')):\n",
    "                if not 'XXXXXXXXXX' in entry.seq:   #ditch the weird sequence\n",
    "                    organelle_seqs.write('>phytoref_chloroplast_' + str(i) + '\\n')\n",
    "                    organelle_seqs.write(str(entry.seq + '\\n'))\n",
    "                    specific_info = str(entry.description).split('|')[-1]\n",
    "                    silva_taxonomy.write('phytoref_chloroplast_' + str(i) + '\\td__Bacteria; p__Cyanobacteria; c__Cyanobacteriia; o__Chloroplast; f__Chloroplast; g__Chloroplast; s__' + specific_info + '\\n')\n",
    "                    gg_taxonomy.write('phytoref_chloroplast_' + str(i) + '\\tk__Bacteria; p__Cyanobacteria; c__Chloroplast; o__Chloroplast; f__Chloroplast; g__Chloroplast; s__' + specific_info + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tanya/Work_files/organelle_removal/output/taxonomy_references/gg_extended_sequences.qza'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import, select V4 region, merge, save\n",
    "organelle_seqs = Artifact.import_data('FeatureData[Sequence]',\n",
    "                                      refs_dir + '/organelle_sequences.fasta')\n",
    "v4_organelle_seqs, = extract_reads(organelle_seqs, 'GTGYCAGCMGCCGCGGTAA',\n",
    "                                   'GGACTACNVGGGTWTCTAAT', n_jobs = 24,\n",
    "                                   read_orientation = 'forward')\n",
    "silva_extended_seqs, = merge_seqs([v4_organelle_seqs,\n",
    "                                   Artifact.load(refs_dir +\n",
    "                                                 '/silva_sequences.qza')])\n",
    "#save the sequence files for both extended files\n",
    "silva_extended_seqs.save(refs_dir + '/silva_extended_sequences.qza')\n",
    "gg_seqs = Artifact.import_data('FeatureData[Sequence]', refs_dir +\n",
    "                               '/gg_13_8_otus/rep_set/99_otus.fasta')\n",
    "v4_gg_seqs, = extract_reads(gg_seqs, 'GTGYCAGCMGCCGCGGTAA',\n",
    "                            'GGACTACNVGGGTWTCTAAT', n_jobs = 24,\n",
    "                            read_orientation = 'forward')\n",
    "v4_gg_seqs.save(refs_dir + '/gg_sequences.qza')\n",
    "gg_extended_seqs, = merge_seqs([organelle_seqs, gg_seqs])\n",
    "gg_extended_seqs.save(refs_dir + '/gg_extended_sequences.qza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tanya/Work_files/organelle_removal/output/taxonomy_references/gg_extended_taxonomy.qza'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the taxonomy files for both extended files\n",
    "silva_organelle_taxonomy = Artifact.import_data('FeatureData[Taxonomy]',\n",
    "                                                refs_dir +\n",
    "                                                '/silva_organelle_taxonomy.tsv')\n",
    "silva_extended_taxonomy, = merge_taxa([silva_organelle_taxonomy,\n",
    "                                       Artifact.load(refs_dir +\n",
    "                                                     '/silva_taxonomy.qza')])\n",
    "silva_extended_taxonomy.save(refs_dir + '/silva_extended_taxonomy.qza')\n",
    "gg_taxonomy = Artifact.import_data('FeatureData[Taxonomy]', refs_dir +\n",
    "                                   '/gg_13_8_otus/taxonomy/99_otu_taxonomy.txt',\n",
    "                                   'HeaderlessTSVTaxonomyFormat')\n",
    "gg_taxonomy.save(refs_dir + '/gg_taxonomy.qza')\n",
    "gg_organelle_taxonomy = Artifact.import_data('FeatureData[Taxonomy]',\n",
    "                                             refs_dir +\n",
    "                                             '/gg_organelle_taxonomy.tsv')\n",
    "gg_extended_taxonomy, = merge_taxa([gg_organelle_taxonomy, gg_taxonomy])\n",
    "gg_extended_taxonomy.save(refs_dir + '/gg_extended_taxonomy.qza')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify taxonomy with vsearch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the directories and import files for this part of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = abspath('..')\n",
    "\n",
    "#import the metadata and sequence files from your analysis\n",
    "metadata_path = working_dir + '/input/sample_metadata_live_vs_dead_combo.tsv'\n",
    "seqs_path = working_dir + '/input/rep_seqs_merged.qza'\n",
    "\n",
    "#import the taxonomy files created in the previously created step.\n",
    "taxonomy_reference_dir = working_dir + '/output/taxonomy_references/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that all files exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying that all needed starting data files exist.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying that all needed starting data files exist.\")\n",
    "for existing_file in [working_dir,metadata_path,seqs_path,taxonomy_reference_dir]:\n",
    "    if not os.path.exists(existing_file):\n",
    "        raise IOError(f\"Required file {existing_file} not found. Please ensure it is in that directory.\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use vsearch to annotate taxonomy. This will be done once for each of the refernce taxonomies created in the last section: Greengenes, Silva, Greengenes + MeTaxa2 + phytoref reference mitochondrial sequences, Silva + MeTaxa2 + phytoref reference mitocondrial sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This step can take a wile to run. It is recommended to run this step overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = ['gg','silva','gg_extended','silva_extended']\n",
    "metadata = Metadata.load(metadata_path)\n",
    "seqs = Artifact.load(seqs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2-archive-6ri4ri5f/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2-archive-5j06xdmi/9d299643-4922-44bc-8003-3d5185d76805/data/dna-sequences.fasta --threads 4 --output_no_hits --blast6out /tmp/tmpirqojtnn\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_linux_x86_64, 15.5GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2-archive-5j06xdmi/9d299643-4922-44bc-8003-3d5185d76805/data/dna-sequences.fasta 100%\n",
      "51405917 nt in 202865 seqs, min 81, max 1003, avg 253\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching query sequences: 0 of 464 (0.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2-archive-6ri4ri5f/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2-archive-mjgxjjvw/b41681fb-a4e7-4ef8-a23a-a26f1bcfd272/data/dna-sequences.fasta --threads 4 --output_no_hits --blast6out /tmp/tmpqgopdrng\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_linux_x86_64, 15.5GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2-archive-mjgxjjvw/b41681fb-a4e7-4ef8-a23a-a26f1bcfd272/data/dna-sequences.fasta 100%\n",
      "86453445 nt in 313734 seqs, min 54, max 2366, avg 276\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching query sequences: 127 of 464 (27.37%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2-archive-6ri4ri5f/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2-archive-ewumxl2w/327da406-e7db-4898-931f-df01072fcefd/data/dna-sequences.fasta --threads 4 --output_no_hits --blast6out /tmp/tmpr3s8ims5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_linux_x86_64, 15.5GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2-archive-ewumxl2w/327da406-e7db-4898-931f-df01072fcefd/data/dna-sequences.fasta 100%\n",
      "303450669 nt in 213319 seqs, min 46, max 5604, avg 1423\n",
      "minseqlength 32: 1 sequence discarded.\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching query sequences: 373 of 464 (80.39%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2-archive-6ri4ri5f/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2-archive-4cipr0id/5a4b411f-26bc-4a88-9a05-7499d740c944/data/dna-sequences.fasta --threads 4 --output_no_hits --blast6out /tmp/tmpbso9jlee\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_linux_x86_64, 15.5GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2-archive-4cipr0id/5a4b411f-26bc-4a88-9a05-7499d740c944/data/dna-sequences.fasta 100%\n",
      "88535911 nt in 322907 seqs, min 54, max 2366, avg 274\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching query sequences: 127 of 464 (27.37%)\n"
     ]
    }
   ],
   "source": [
    "vsearch_results = {}\n",
    "for reference in references:\n",
    "    reference_otu_path = taxonomy_reference_dir + f'{reference}_sequences.qza'\n",
    "    reference_taxonomy_path = taxonomy_reference_dir + f'{reference}_taxonomy.qza'\n",
    "    reads = Artifact.load(reference_otu_path)\n",
    "    taxonomy = Artifact.load(reference_taxonomy_path)\n",
    "    vsearch_results[reference] = classify_consensus_vsearch(seqs, reads, taxonomy, threads = 4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save each of the taxonomy annotations for your sample sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reference in vsearch_results:\n",
    "    classification_taxonomy, = vsearch_results[reference]\n",
    "    classification_taxonomy.save(working_dir + '/output/' + str(reference) + '_reference_taxonomy.qza')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of mitochondria from samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up needed files for removal of mitochondria from samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filepath = abspath(\"../output\")\n",
    "input_filepath = abspath(\"../input\")\n",
    "output_dir = abspath(\"../output/filtered_tables\")\n",
    "\n",
    "#These 3 files are specific to your study\n",
    "feature_table_dir = working_dir + '/input/feature_table_live_vs_dead.qza'\n",
    "mapping_file = metadata_path\n",
    "sequence_file = seqs_path\n",
    "\n",
    "#Load the taxonomy files created in the last section.\n",
    "taxonomy_files = {\"greengenes_reference\":\"../output/gg_reference_taxonomy.qza\",\\\n",
    "                  \"greengenes_extended\": \"../output/gg_extended_reference_taxonomy.qza\",\\\n",
    "                  \"silva_reference\": \"../output/silva_reference_taxonomy.qza\",\\\n",
    "                  \"silva_extended\": \"../output/silva_extended_reference_taxonomy.qza\"}\n",
    "\n",
    "required_files = [feature_table,mapping_file,sequence_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all files exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying that all needed starting data files exist.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying that all needed starting data files exist.\")\n",
    "for existing_files in required_files:\n",
    "    if not exists(existing_file):\n",
    "        raise IOError(f\"Required file {existing_file} not found. Please ensure it is in the directory.\")\n",
    "        \n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate filtered tables using the default taxonomies for Greengenes and Silva as well as those supplemented with metaxa2 mitochondrial 16S rRNA and phytoref chloroplasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data using the greengenes_reference taxonomy (../output/gg_reference_taxonomy.qza)\n",
      "Removing mitochondia from: <artifact: FeatureTable[Frequency] uuid: 97994d43-af84-4dcd-a4a5-316bcc00a472>\n",
      "Saving results to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_greengenes_reference_mws.qza\n",
      "Saving file summary to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_greengenes_reference_mws.qzv\n",
      "Done with processing greengenes_reference taxonomy annotations!\n",
      "\n",
      "\n",
      "Analyzing data using the greengenes_extended taxonomy (../output/gg_extended_reference_taxonomy.qza)\n",
      "Removing mitochondia from: <artifact: FeatureTable[Frequency] uuid: 97994d43-af84-4dcd-a4a5-316bcc00a472>\n",
      "Saving results to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_greengenes_extended_mws.qza\n",
      "Saving file summary to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_greengenes_extended_mws.qzv\n",
      "Done with processing greengenes_extended taxonomy annotations!\n",
      "\n",
      "\n",
      "Analyzing data using the silva_reference taxonomy (../output/silva_reference_taxonomy.qza)\n",
      "Removing mitochondia from: <artifact: FeatureTable[Frequency] uuid: 97994d43-af84-4dcd-a4a5-316bcc00a472>\n",
      "Saving results to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_silva_reference_mws.qza\n",
      "Saving file summary to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_silva_reference_mws.qzv\n",
      "Done with processing silva_reference taxonomy annotations!\n",
      "\n",
      "\n",
      "Analyzing data using the silva_extended taxonomy (../output/silva_extended_reference_taxonomy.qza)\n",
      "Removing mitochondia from: <artifact: FeatureTable[Frequency] uuid: 97994d43-af84-4dcd-a4a5-316bcc00a472>\n",
      "Saving results to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_silva_extended_mws.qza\n",
      "Saving file summary to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_silva_extended_mws.qzv\n",
      "Done with processing silva_extended taxonomy annotations!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_feature_tables_by_taxonomy = defaultdict(dict)\n",
    "\n",
    "feature_table = Artifact.load(feature_table_dir)\n",
    "\n",
    "for label, taxonomy_file in taxonomy_files.items():\n",
    "    print(f\"Analyzing data using the {label} taxonomy ({taxonomy_file})\")\n",
    "    taxonomy = Artifact.load(taxonomy_file)\n",
    "    print(f\"Removing mitochondia from: {feature_table}\")\n",
    "    #Qiime2 API does not return a single object, but a named Tuple struture with each output.\n",
    "    filter_table_results = filter_table(feature_table,taxonomy,exclude=\"mitochondria,chloroplast\",mode=\"contains\")\n",
    "    filter_table_results = filter_samples(filter_table_results.filtered_table,metadata=metadata)\n",
    "    filtered_table = filter_table_results.filtered_table\n",
    "    \n",
    "    #save the file\n",
    "    output_filename = f\"feature_table_filtered_{label}_mws.qza\"\n",
    "    output_filepath = join(output_dir,output_filename)\n",
    "    print(f\"Saving results to: {output_filepath}\")\n",
    "    filtered_table.save(output_filepath)\n",
    "    \n",
    "    #output a file summary\n",
    "    summary_visualization = summarize(filtered_table,sample_metadata=metadata)\n",
    "    vis = summary_visualization.visualization\n",
    "    output_filename = f\"feature_table_filtered_{label}_mws.qzv\"\n",
    "    output_filepath = join(output_dir,output_filename)\n",
    "    print(f\"Saving file summary to: {output_filepath}\")\n",
    "    \n",
    "    filtered_feature_tables_by_taxonomy[label]=filtered_table\n",
    "    print(f\"Done with processing {label} taxonomy annotations!\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rarefy tables to an even depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: 44d128c1-3928-45c1-812b-5b4951168af0> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n",
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: 198b8748-a810-412f-b31a-ec9bbdd5def8> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n",
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: c6323e2b-0a78-4e72-939e-08cc227a6085> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n",
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: 5bb410e1-75b4-41c1-8887-073b4db4ac1e> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n"
     ]
    }
   ],
   "source": [
    "#choose a rarefaction depth appropriate for your study.\n",
    "rarefaction_depth = 1000\n",
    "\n",
    "rarefied_feature_tables_by_taxonomy = defaultdict(dict)\n",
    "\n",
    "for label,filtered_feature_tables in filtered_feature_tables_by_taxonomy.items():\n",
    "    print(f\"Rarefying feature table {filtered_feature_tables} to {rarefaction_depth} sequences/sample.\")\n",
    "    rarefy_results = rarefy(table=filtered_feature_tables,sampling_depth=rarefaction_depth)\n",
    "    #get the rarefied table out of the NamedTuple of results\n",
    "    rarefied_filtered_table = rarefy_results.rarefied_table\n",
    "        \n",
    "    #save the resulting feature table\n",
    "    output_filename = f\"feature_table_{label}_{rarefaction_depth}.qza\"\n",
    "    output_filepath = join(output_dir,output_filename)\n",
    "    print(\"Saving results to:{output_filepath}\")\n",
    "    rarefied_filtered_table.save(output_filepath)\n",
    "        \n",
    "    #store the rarefied tables in a dict so they don't need to relode them\n",
    "    rarefied_feature_tables_by_taxonomy[label] = rarefied_filtered_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
