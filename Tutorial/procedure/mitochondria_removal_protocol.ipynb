{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitochondria removal tutorial (QIIME2 Artifact API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will cover how to use the preconstructed extended taxonomies from Sonett et al. [preprint](https://www.biorxiv.org/content/10.1101/2021.02.23.431501v2) to remove mitochondria from 16S rRNA gene sequence data in QIIME2 using the QIIME2 Artifact API\n",
    "\n",
    "If you need to build your own extended taxonomy, see the extended [taxonomy creation tutorial here](extended_taxonomy_construction_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "\n",
    "It's assumed that QIIME2 is installed. You can run this tutorial from a Jupyter notebook in Terminal on Mac, any BASH command line interface in Linux or the Windows Subsystem for Linux, or PowerShell on Windows.\n",
    "\n",
    "It's also assumed that you've downloaded the zipped tutorial with `input`, `output` and `procedure` folders, and that this script is within the provided procedure folder. \n",
    "\n",
    "Given all that, this tutorial will discuss how to use the the supplemented databases to remove mitochondria from your 16S datasets using Qiime2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from qiime2 import Artifact, Metadata\n",
    "except ModuleNotFoundError:\n",
    "    raise ModuleNotFoundError('\\nQIIME2 is not installed, or you are not in the proper environment.\\nPlease stop the Jupyter server, install QIIME2 or activate the environment, and restart this notebook.') from None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify taxonomy with VSEARCH\n",
    "In order to remove mitochondria and chloroplast sequences, we must first classify the taxonomy of all 16S rRNA sequences in the library to figure out which derive from organelles. In this tutorial, we'll use VSEARCH to align our 16S sequences to the extended reference database. VSEARCH is included with QIIME2 and so you shouldn't need to install any additional software to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the directories and import files for this part of the analysis\n",
    "\n",
    "First we'll set up references to the directories that hold our raw data and results. The tutorial assumes that we have an `input`, `output`, and `procedure` folder within our analysis directory, and that we are running our jupyter notebook from the `procedure` folder. Therefore all paths to files in `input` will look like `../input/name_of_file.txt` and similarly files in the `output` folder will have paths like `../output/name_of_file.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by assigning some relevant paths to variables. If you were adapting this tutorial to your own data, you would need to replace the filenames for the metadata, sequences, and feature table to your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata filepath:/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/sample_metadata_live_vs_dead_combo.tsv\n",
      "\n",
      "Sequence filepath:/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/rep_seqs_merged.qza\n",
      "\n",
      "Feature table filepath:/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/feature_table_live_vs_dead.qza\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os.path import join, abspath, exists\n",
    "\n",
    "#Since we are in procedure, the main analysis directory ('working_dir')\n",
    "#encloses our current directory\n",
    "working_dir = abspath('..')\n",
    "\n",
    "input_dir = join(working_dir,'input')\n",
    "output_dir = join(working_dir, 'output')\n",
    "\n",
    "#Define variables to hold the filepaths for metadata, sequences, and the extended taxonomy reference\n",
    "\n",
    "#NOTE: these should be adjusted to point to your data\n",
    "metadata_file_name = 'sample_metadata_live_vs_dead_combo.tsv'\n",
    "sequence_file_name = 'rep_seqs_merged.qza'\n",
    "feature_table_name = 'feature_table_live_vs_dead.qza'\n",
    "\n",
    "#Combine the filenames with paths for your input folders\n",
    "#We use the join function to ensure cross-system compatibility (i.e. / vs. \\ issues)\n",
    "metadata_path = join(input_dir, metadata_file_name)\n",
    "seqs_path = join(input_dir, sequence_file_name)\n",
    "feature_table_path = join(input_dir, feature_table_name)\n",
    "\n",
    "print(f\"Metadata filepath:{metadata_path}\\n\")\n",
    "print(f\"Sequence filepath:{seqs_path}\\n\")\n",
    "print(f\"Feature table filepath:{feature_table_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables holding the paths to our taxonomic references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these will be automaticallly downloaded if not found\n",
    "taxonomy_reference_dir = join(input_dir,'taxonomy_references')\n",
    "base_silva_paths = [join(taxonomy_reference_dir, 'silva_sequences.qza'),\n",
    "                    join(taxonomy_reference_dir, 'silva_taxonomy.qza')]\n",
    "extended_silva_paths = [join(taxonomy_reference_dir, 'silva_extended_sequences.qza'),\n",
    "                        join(taxonomy_reference_dir, 'silva_extended_taxonomy.qza')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for files and download key files if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "\n",
    "def download_file(url, local_filepath):\n",
    "    with urllib.request.urlopen(url) as response, open(local_filepath, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying that all needed starting data files exist.\n",
      "/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/taxonomy_references.....OK\n",
      "/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/sample_metadata_live_vs_dead_combo.tsv.....OK\n",
      "/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/rep_seqs_merged.qza.....OK\n",
      "/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/feature_table_live_vs_dead.qza.....OK\n",
      "/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/taxonomy_references/silva_sequences.qza.....OK\n",
      "/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/taxonomy_references/silva_taxonomy.qza.....OK\n",
      "/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/taxonomy_references/silva_extended_sequences.qza.....OK\n",
      "/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/taxonomy_references/silva_extended_taxonomy.qza.....OK\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying that all needed starting data files exist.\")\n",
    "\n",
    "### Add file names for the actual files we need to the required files list\n",
    "\n",
    "required_filepaths = ([taxonomy_reference_dir, metadata_path, seqs_path, feature_table_path]\n",
    "                      + base_silva_paths + extended_silva_paths)\n",
    "\n",
    "for existing_file in required_filepaths:\n",
    "    if not exists(existing_file):\n",
    "        if existing_file == taxonomy_reference_dir:\n",
    "            os.mkdir(taxonomy_reference_dir)\n",
    "        elif existing_file in base_silva_paths:\n",
    "            download_file('https://data.qiime2.org/2021.4/common/silva-138-99-seqs-515-806.qza',\n",
    "                          join(taxonomy_reference_dir, 'silva_sequences.qza'))\n",
    "            download_file('https://data.qiime2.org/2021.4/common/silva-138-99-tax-515-806.qza', \n",
    "                          join(taxonomy_reference_dir, 'silva_taxonomy.qza'))\n",
    "        elif existing_file in extended_silva_paths:\n",
    "            download_file('https://zenodo.org/records/10251912/files/silva_extended_sequences.qza?download=1',\n",
    "                          join(taxonomy_reference_dir, 'silva_extended_sequences.qza'))\n",
    "            download_file('https://zenodo.org/records/10251912/files/silva_extended_taxonomy.qza?download=1',\n",
    "                          join(taxonomy_reference_dir, 'silva_extended_taxonomy.qza'))            \n",
    "        else:\n",
    "            raise IOError(f\"Required file {existing_file} not found. Please ensure it is in that directory.\")\n",
    "        \n",
    "    print(f\"{existing_file}.....OK\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use vsearch to annotate taxonomy. This will be done once for each of the refernce taxonomies :  Silva, and Silva + MeTaxa2 + phytoref reference mitocondrial sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata and sequence files as .qza artifacts\n",
    "\n",
    "Next we'll load your study-specific metadata and sequence files as QIIME2 Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiime2 import Artifact, Metadata\n",
    "metadata = Metadata.load(metadata_path)\n",
    "seqs = Artifact.load(seqs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use VSEARCH to classify your sequences according to base and extended SILVA taxonomies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** On a full dataset this step can take a while to run. Adjusting the vsearch threads parameter can help speed up the process if you have enough memory to support multiple threads (about 8GB / thread has worked for us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see https://forum.qiime2.org/t/vsearch-classifier-memory/8667/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2/dylan/data/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2/dylan/data/b41681fb-a4e7-4ef8-a23a-a26f1bcfd272/data/dna-sequences.fasta --threads 1 --output_no_hits --blast6out /tmp/q2-BLAST6Format-vnb3nekd\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.22.1_linux_x86_64, 7.7GB RAM, 12 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2/dylan/data/b41681fb-a4e7-4ef8-a23a-a26f1bcfd272/data/dna-sequences.fasta 100%\n",
      "86453445 nt in 313734 seqs, min 54, max 2366, avg 276\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching unique query sequences: 127 of 464 (27.37%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2/dylan/data/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2/dylan/data/f5bba9f9-ccf6-4f7f-9610-aa9dcdb47eca/data/dna-sequences.fasta --threads 1 --output_no_hits --blast6out /tmp/q2-BLAST6Format-jbz41lto\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.22.1_linux_x86_64, 7.7GB RAM, 12 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2/dylan/data/f5bba9f9-ccf6-4f7f-9610-aa9dcdb47eca/data/dna-sequences.fasta 100%\n",
      "88535151 nt in 322903 seqs, min 54, max 2366, avg 274\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching unique query sequences: 127 of 464 (27.37%)\n"
     ]
    }
   ],
   "source": [
    "from qiime2.plugins.feature_classifier.pipelines import classify_consensus_vsearch\n",
    "\n",
    "threads = 1\n",
    "\n",
    "\n",
    "vsearch_results = {}\n",
    "references = ['silva','silva_extended']\n",
    "for reference in references:\n",
    "    \n",
    "    #Note: The next two lines just set paths to the sequence and taxonomy qza files, respectively\n",
    "    #If using a custom reference set, you could either name it with the same naming scheme \n",
    "    # my_reference_sequences.qza and my_reference_taxonomy.qza, and add 'my_reference' to the \n",
    "    #references list up above, or just manually set the file names using this section as a \n",
    "    #loose guide.\n",
    "    \n",
    "    reference_otu_path = join(taxonomy_reference_dir, f'{reference}_sequences.qza')\n",
    "    reference_taxonomy_path = join(taxonomy_reference_dir, f'{reference}_taxonomy.qza')\n",
    "    \n",
    "    #Load .qza files as QIIME2 artifacts\n",
    "    reads = Artifact.load(reference_otu_path)\n",
    "    taxonomy = Artifact.load(reference_taxonomy_path)\n",
    "    \n",
    "    #Run VSEARCH, and store results in the vsearch_results dictionary\n",
    "    vsearch_results[reference] = classify_consensus_vsearch(seqs, reads, taxonomy, threads = threads)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save each of the taxonomy annotations for your sequences.\n",
    "\n",
    "Finally, save the results of the vsearch taxonomy mapping into taxonomy .qza objects that you can use with downstream QIIME2 scripts. Note that these are *study-specific* classifications of your sequences, unlike the reference taxonomies, which are general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reference in vsearch_results:\n",
    "    classification_taxonomy = vsearch_results[reference].classification\n",
    "    \n",
    "    #Create a .qza file to hold the results of applying a given reference taxonomy\n",
    "    #to your specific sequences (as distinct from the *reference* taxonomy for all known species)\n",
    "    output_filepath = join(working_dir,'output',f\"{reference}_classification_taxonomy.qza\")\n",
    "    classification_taxonomy.save(output_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove mitochondria from samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created study-specific taxonomic annotations in the output folder (e.g. `../output/silva_extended_classification_taxonomy.qza`), we can use them to filter our feature tables to remove mitochondria or chloroplast 16S sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up filepaths\n",
    "\n",
    "First we'll set up variables to hold the filepaths we'll need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#already done up top -- I think this is a duplicated cell; variables aren't called below\n",
    "\n",
    "# output_dir = abspath(\"../output\")\n",
    "# input_filepath = abspath(\"../input\")\n",
    "#mapping_file = metadata_path\n",
    "#sequence_file = seqs_path\n",
    "\n",
    "\n",
    "#These files are specific to your study\n",
    "feature_table_name = 'feature_table_live_vs_dead.qza'\n",
    "feature_table_path = join(input_dir, feature_table_name)\n",
    "\n",
    "\n",
    "\n",
    "#Load the taxonomy files created in the last section.\n",
    "taxonomy_files = {\"silva_base\": \"../output/silva_reference_taxonomy.qza\",\\\n",
    "                  \"silva_extended\": \"../output/silva_extended_reference_taxonomy.qza\"}\n",
    "\n",
    "required_files = [feature_table,mapping_file,sequence_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate filtered feature tables tables with organelle sequences removed.\n",
    "\n",
    "Next we'll use the QIIME2 filter_table command to remove features that are annotated as mitochondria or chloroplasts according to each taxonomy, and output filtered feature tables and feature table summaries for each to allow comparison and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewrote this cell, keeping for reference atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# filtered_feature_tables_by_taxonomy = defaultdict(dict)\n",
    "\n",
    "# feature_table = Artifact.load(feature_table_path)\n",
    "\n",
    "# for label, taxonomy_file in taxonomy_files.items():\n",
    "#     print(f\"Analyzing data using the {label} taxonomy ({taxonomy_file})\")\n",
    "#     taxonomy = Artifact.load(taxonomy_file)\n",
    "#     print(f\"Removing mitochondia from: {feature_table}\")\n",
    "\n",
    "#     #Remove organelle sequeces from the feature table\n",
    "#     #Note that the Qiime2 API does not return a single object, \n",
    "#     #but rather a named Tuple struture with each output, which we save in filter_table_results\n",
    "#     filter_table_results = filter_table(feature_table,taxonomy,exclude=\"mitochondria,chloroplast\",mode=\"contains\")\n",
    "    \n",
    "#     #Additionally remove any samples not in the metadata\n",
    "#     filter_table_results = filter_samples(filter_table_results.filtered_table,metadata=metadata)\n",
    "#     filtered_table = filter_table_results.filtered_table\n",
    "    \n",
    "#     #Save the filtered feature table to a .qza file in the output directory\n",
    "#     output_filename = f\"feature_table_filtered_{label}_mws.qza\"\n",
    "#     output_filepath = join(output_dir,output_filename)\n",
    "#     print(f\"Saving results to: {output_filepath}\")\n",
    "#     filtered_table.save(output_filepath)\n",
    "    \n",
    "#     #Output a feature table summary visualization\n",
    "#     summary_visualization = summarize(filtered_table,sample_metadata=metadata)\n",
    "#     vis = summary_visualization.visualization\n",
    "#     output_filename = f\"feature_table_filtered_{label}_mws.qzv\"\n",
    "#     output_filepath = join(output_dir,output_filename)\n",
    "#     print(f\"Saving file summary to: {output_filepath}\")\n",
    "    \n",
    "#     filtered_feature_tables_by_taxonomy[label]=filtered_table\n",
    "#     print(f\"Done with processing {label} taxonomy annotations!\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data using the silva classification taxonomy\n",
      "Removing mitochondria from: feature_table_live_vs_dead.qza\n",
      "Saving results to: /mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/output/feature_table_filtered_silva.qza\n",
      "Saving file summary to: /mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/output/feature_table_filtered_silva.qzv\n",
      "\n",
      "Analyzing data using the silva_extended classification taxonomy\n",
      "Removing mitochondria from: feature_table_live_vs_dead.qza\n",
      "Saving results to: /mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/output/feature_table_filtered_silva_extended.qza\n",
      "Saving file summary to: /mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/output/feature_table_filtered_silva_extended.qzv\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from qiime2.plugins.feature_table.methods import filter_samples\n",
    "from qiime2.plugins.feature_table.visualizers import summarize\n",
    "from qiime2.plugins.taxa.methods import filter_table\n",
    "\n",
    "filtered_feature_tables_by_taxonomy = defaultdict(dict)\n",
    "\n",
    "feature_table = Artifact.load(feature_table_path)\n",
    "\n",
    "for reference, classification_taxonomy in vsearch_results.items():\n",
    "    print(f'Analyzing data using the {reference} classification taxonomy')\n",
    "    print(f'Removing mitochondria from: {feature_table_name}')\n",
    "    \n",
    "    #Remove organelle sequeces from the feature table\n",
    "    #Note that the Qiime2 API does not return a single object, \n",
    "    #but rather a named Tuple struture with each output, which we save in filter_table_results\n",
    "    filter_table_results = filter_table(table = feature_table, taxonomy = classification_taxonomy.classification,\n",
    "                                        exclude = 'mitochondria,chloroplast',\n",
    "                                        mode = 'contains')\n",
    "    #Additionally remove any samples not in the metadata\n",
    "    filter_table_results = filter_samples(filter_table_results.filtered_table,\n",
    "                                          metadata = metadata)\n",
    "    filtered_table = filter_table_results.filtered_table\n",
    "    \n",
    "    #Save the filtered feature table to a .qza file in the output directory\n",
    "    filtered_table_path = join(output_dir, f'feature_table_filtered_{reference}.qza')\n",
    "    print(f'Saving results to: {filtered_table_path}')\n",
    "    filtered_table.save(filtered_table_path)\n",
    "    \n",
    "    #output a feature table summary visualization\n",
    "    summary_visualization = summarize(filtered_table, sample_metadata = metadata)\n",
    "    vis = summary_visualization.visualization\n",
    "    visualization_path = join(output_dir, f'feature_table_filtered_{reference}.qzv')\n",
    "    print(f'Saving file summary to: {visualization_path}\\n')\n",
    "    vis.save(visualization_path)\n",
    "print('Done.')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rarefy tables to an even depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do we want to include this rarefaction step? I assume we want to replace as little of their pipeline as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: 44d128c1-3928-45c1-812b-5b4951168af0> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n",
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: 198b8748-a810-412f-b31a-ec9bbdd5def8> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n",
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: c6323e2b-0a78-4e72-939e-08cc227a6085> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n",
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: 5bb410e1-75b4-41c1-8887-073b4db4ac1e> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n"
     ]
    }
   ],
   "source": [
    "#choose a rarefaction depth appropriate for your study.\n",
    "rarefaction_depth = 1000\n",
    "\n",
    "rarefied_feature_tables_by_taxonomy = {}\n",
    "\n",
    "for label, filtered_feature_tables in filtered_feature_tables_by_taxonomy.items():\n",
    "    print(f'Rarefying feature table {filtered_feature_tables} to {rarefaction_depth} sequences/sample.')\n",
    "    rarefy_results = rarefy(table=filtered_feature_tables,sampling_depth=rarefaction_depth)\n",
    "    #get the rarefied table out of the NamedTuple of results\n",
    "    rarefied_filtered_table = rarefy_results.rarefied_table\n",
    "        \n",
    "    #save the resulting feature table\n",
    "    output_filename = f'feature_table_{label}_{rarefaction_depth}.qza'\n",
    "    output_filepath = join(output_dir,output_filename)\n",
    "    print(f'Saving results to:{output_filepath}')\n",
    "    rarefied_filtered_table.save(output_filepath)\n",
    "        \n",
    "    #store the rarefied tables in a dict so they don't need to relode them\n",
    "    rarefied_feature_tables_by_taxonomy[label] = rarefied_filtered_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
