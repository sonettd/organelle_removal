{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitochondria removal protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This tutorial will cover how to use the preconstructed extended taxonomies from Sonett et al. to remove mitochondria from 16S rRNA gene sequence data in QIIME2.\n",
    "\n",
    "If you need to build your own extended taxonomy, see the extended [taxonomy creation tutorial here](extended_taxonomy_construction_tutorial.ipynb)\n",
    "\n",
    "The tutorial assumes that you have a top-level analysis folder, and within it an `input` `output` and `procedure` folder, and that this script is located in the procedure folder. Therefore, whenever we refer to files in input it will be with relative paths like `../input/name_of_some_file.txt`. You can either create these in your finder, or on the command line. The exact command will vary by command line interface, but on BASH or PowerShell you can type `mkdir input` to create an input directory in your current folder, `mkdir output` to create an output directory in your current folder, etc.\n",
    "\n",
    "It's further assumed that QIIME2 is installed.\n",
    "\n",
    "Given all that, this tutorial will discuss how to use the the supplemented databases to remove mitochondria from your 16S datasets using Qiime2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify taxonomy with vsearch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the directories and import files for this part of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, abspath, exists\n",
    "working_dir = abspath('..')\n",
    "input_dir = join(working_dir,'input')\n",
    "output_dir = join(working_dir, 'output')\n",
    "\n",
    "\n",
    "#Define variables to hold the filepaths for metadata, sequences, and the extended taxonomy reference\n",
    "\n",
    "#these should be adjusted to point to your data\n",
    "metadata_file_name = 'sample_metadata_live_vs_dead_combo.tsv'\n",
    "sequence_file_name = 'rep_seqs_merged.qza'\n",
    "feature_table_name = 'feature_table_live_vs_dead.qza'\n",
    "\n",
    "metadata_path = join(input_dir, metadata_file_name)\n",
    "seqs_path = join(input_dir, sequence_file_name)\n",
    "feature_table_path = join(input_dir, feature_table_name)\n",
    "\n",
    "#these will be automaticallly downloaded if not found\n",
    "taxonomy_reference_dir = join(input_dir,'taxonomy_references')\n",
    "base_silva_paths = [join(taxonomy_reference_dir, 'silva_sequences.qza'),\n",
    "                    join(taxonomy_reference_dir, 'silva_taxonomy.qza')]\n",
    "extended_silva_paths = [join(taxonomy_reference_dir, 'silva_extended_sequences.qza'),\n",
    "                        join(taxonomy_reference_dir, 'silva_extended_taxonomy.qza')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that all files exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "\n",
    "def download_file(url, local_filepath):\n",
    "    with urllib.request.urlopen(url) as response, open(local_filepath, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is being troublesome ... thoughts?\n",
    "https://stackoverflow.com/questions/34692009/download-image-from-url-using-python-urllib-but-receiving-http-error-403-forbid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying that all needed starting data files exist.\n",
      "/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/taxonomy_references.....OK\n",
      "/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/sample_metadata_live_vs_dead_combo.tsv.....OK\n",
      "/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/rep_seqs_merged.qza.....OK\n",
      "/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/taxonomy_references/silva_sequences.qza.....OK\n",
      "/mnt/c/Users/dsone/Documents/zaneveld/organelle_removal/organelle_removal/Tutorial/input/taxonomy_references/silva_taxonomy.qza.....OK\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     download_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://data.qiime2.org/2021.4/common/silva-138-99-tax-515-806.qza\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     15\u001b[0m                   join(taxonomy_reference_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msilva_taxonomy.qza\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m existing_file \u001b[38;5;129;01min\u001b[39;00m extended_silva_paths:\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://zenodo.org/records/10251912/files/silva_extended_sequences.qza?download=1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaxonomy_reference_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msilva_extended_sequences.qza\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     download_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://zenodo.org/records/10251912/files/silva_extended_taxonomy.qza?download=1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m                   join(taxonomy_reference_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msilva_extended_taxonomy.qza\u001b[39m\u001b[38;5;124m'\u001b[39m))            \n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(url, local_filepath)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_file\u001b[39m(url, local_filepath):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m response, \u001b[38;5;28mopen\u001b[39m(local_filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m out_file:\n\u001b[1;32m      7\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mcopyfileobj(response, out_file)\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "print(\"Verifying that all needed starting data files exist.\")\n",
    "\n",
    "### Add file names for the actual files we need to the required files list\n",
    "\n",
    "required_filepaths = ([taxonomy_reference_dir, metadata_path, seqs_path, feature_table_path]\n",
    "                      + base_silva_paths + extended_silva_paths)\n",
    "\n",
    "for existing_file in required_filepaths:\n",
    "    if not exists(existing_file):\n",
    "        if existing_file == taxonomy_reference_dir:\n",
    "            os.mkdir(taxonomy_reference_dir)\n",
    "        elif existing_file in base_silva_paths:\n",
    "            download_file('https://data.qiime2.org/2021.4/common/silva-138-99-seqs-515-806.qza',\n",
    "                          join(taxonomy_reference_dir, 'silva_sequences.qza'))\n",
    "            download_file('https://data.qiime2.org/2021.4/common/silva-138-99-tax-515-806.qza', \n",
    "                          join(taxonomy_reference_dir, 'silva_taxonomy.qza'))\n",
    "        elif existing_file in extended_silva_paths:\n",
    "            download_file('https://zenodo.org/records/10251912/files/silva_extended_sequences.qza?download=1',\n",
    "                          join(taxonomy_reference_dir, 'silva_extended_sequences.qza'))\n",
    "            download_file('https://zenodo.org/records/10251912/files/silva_extended_taxonomy.qza?download=1',\n",
    "                          join(taxonomy_reference_dir, 'silva_extended_taxonomy.qza'))            \n",
    "        else:\n",
    "            raise IOError(f\"Required file {existing_file} not found. Please ensure it is in that directory.\")\n",
    "        \n",
    "    print(f\"{existing_file}.....OK\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use vsearch to annotate taxonomy. This will be done once for each of the refernce taxonomies :  Silva, and Silva + MeTaxa2 + phytoref reference mitocondrial sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata and sequence files as .qza artifacts\n",
    "\n",
    "Next we'll load your study-specific metadata and sequence files as QIIME2 Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = Metadata.load(metadata_path)\n",
    "seqs = Artifact.load(seqs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use VSEARCH to classify your sequences according to base and extended SILVA taxonomies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** On a full dataset this step can take a while to run. Adjusting the vsearch threads parameter can help speed up the process if you have enough memory to support multiple threads (about 8GB / thread is recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see https://forum.qiime2.org/t/vsearch-classifier-memory/8667/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2-archive-6ri4ri5f/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2-archive-5j06xdmi/9d299643-4922-44bc-8003-3d5185d76805/data/dna-sequences.fasta --threads 4 --output_no_hits --blast6out /tmp/tmpirqojtnn\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_linux_x86_64, 15.5GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2-archive-5j06xdmi/9d299643-4922-44bc-8003-3d5185d76805/data/dna-sequences.fasta 100%\n",
      "51405917 nt in 202865 seqs, min 81, max 1003, avg 253\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching query sequences: 0 of 464 (0.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2-archive-6ri4ri5f/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2-archive-mjgxjjvw/b41681fb-a4e7-4ef8-a23a-a26f1bcfd272/data/dna-sequences.fasta --threads 4 --output_no_hits --blast6out /tmp/tmpqgopdrng\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_linux_x86_64, 15.5GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2-archive-mjgxjjvw/b41681fb-a4e7-4ef8-a23a-a26f1bcfd272/data/dna-sequences.fasta 100%\n",
      "86453445 nt in 313734 seqs, min 54, max 2366, avg 276\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching query sequences: 127 of 464 (27.37%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2-archive-6ri4ri5f/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2-archive-ewumxl2w/327da406-e7db-4898-931f-df01072fcefd/data/dna-sequences.fasta --threads 4 --output_no_hits --blast6out /tmp/tmpr3s8ims5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_linux_x86_64, 15.5GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2-archive-ewumxl2w/327da406-e7db-4898-931f-df01072fcefd/data/dna-sequences.fasta 100%\n",
      "303450669 nt in 213319 seqs, min 46, max 5604, avg 1423\n",
      "minseqlength 32: 1 sequence discarded.\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching query sequences: 373 of 464 (80.39%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running external command line application. This may print messages to stdout and/or stderr.\n",
      "The command being run is below. This command cannot be manually re-run as it will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: vsearch --usearch_global /tmp/qiime2-archive-6ri4ri5f/d7bb1a41-d3e8-465f-be11-90b58e1cf211/data/dna-sequences.fasta --id 0.8 --query_cov 0.8 --strand both --maxaccepts 10 --maxrejects 0 --db /tmp/qiime2-archive-4cipr0id/5a4b411f-26bc-4a88-9a05-7499d740c944/data/dna-sequences.fasta --threads 4 --output_no_hits --blast6out /tmp/tmpbso9jlee\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_linux_x86_64, 15.5GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file /tmp/qiime2-archive-4cipr0id/5a4b411f-26bc-4a88-9a05-7499d740c944/data/dna-sequences.fasta 100%\n",
      "88535911 nt in 322907 seqs, min 54, max 2366, avg 274\n",
      "Masking 100%\n",
      "Counting k-mers 100%\n",
      "Creating k-mer index 100%\n",
      "Searching 100%\n",
      "Matching query sequences: 127 of 464 (27.37%)\n"
     ]
    }
   ],
   "source": [
    "references = ['silva','silva_extended']\n",
    "\n",
    "vsearch_results = {}\n",
    "for reference in references:\n",
    "    \n",
    "    #Note: The next two lines just set paths to the sequence and taxonomy qza files, respectively\n",
    "    #If using a custom reference set, you could either name it with the same naming scheme \n",
    "    # my_reference_sequences.qza and my_reference_taxonomy.qza, and add 'my_reference' to the \n",
    "    #references list up above, or just manually set the file names using this section as a \n",
    "    #loose guide.\n",
    "    \n",
    "    reference_otu_path = join(taxonomy_reference_dir,f'{reference}_sequences.qza')\n",
    "    reference_taxonomy_path = join(taxonomy_reference_dir,f'{reference}_taxonomy.qza')\n",
    "    \n",
    "    #Load .qza files as QIIME2 artifacts\n",
    "    reads = Artifact.load(reference_otu_path)\n",
    "    taxonomy = Artifact.load(reference_taxonomy_path)\n",
    "    \n",
    "    #Run VSEARCH, and store results in the vsearch_results dictionary\n",
    "    vsearch_results[reference] = classify_consensus_vsearch(seqs, reads, taxonomy, threads = 4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save each of the taxonomy annotations for your sequences.\n",
    "\n",
    "Finally, save the results of the vsearch taxonomy mapping into taxonomy .qza objects that you can use with downstream QIIME2 scripts. Note that these are *study-specific* classifications of your sequences, unlike the reference taxonomies, which are general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reference in vsearch_results:\n",
    "    classification_taxonomy, = vsearch_results[reference]\n",
    "    \n",
    "    #Create a .qza file to hold the results of applying a given reference taxonomy\n",
    "    #to your specific sequences (as distinct from the *reference* taxonomy for all known species)\n",
    "    output_filepath = join(working_dir,'output',f\"{reference}_classification_taxonomy.qza\")\n",
    "    classification_taxonomy.save(output_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove mitochondria from samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created study-specific taxonomic annotations in the output folder (e.g. `../output/silva_extended_classification_taxonomy.qza`), we can use them to filter our feature tables to remove mitochondria or chloroplast 16S sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up filepaths\n",
    "\n",
    "First we'll set up variables to hold the filepaths we'll need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#already done up top -- I think this is a duplicated cell; variables aren't called below\n",
    "\n",
    "# output_dir = abspath(\"../output\")\n",
    "# input_filepath = abspath(\"../input\")\n",
    "#mapping_file = metadata_path\n",
    "#sequence_file = seqs_path\n",
    "\n",
    "\n",
    "#These files are specific to your study\n",
    "feature_table_name = 'feature_table_live_vs_dead.qza'\n",
    "feature_table_path = join(input_dir, feature_table_name)\n",
    "\n",
    "\n",
    "\n",
    "#Load the taxonomy files created in the last section.\n",
    "taxonomy_files = {\"silva_base\": \"../output/silva_reference_taxonomy.qza\",\\\n",
    "                  \"silva_extended\": \"../output/silva_extended_reference_taxonomy.qza\"}\n",
    "\n",
    "required_files = [feature_table,mapping_file,sequence_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate filtered feature tables tables with organelle sequences removed.\n",
    "\n",
    "Next we'll use the QIIME2 filter_table command to remove features that are annotated as mitochondria or chloroplasts according to each taxonomy, and output filtered feature tables and feature table summaries for each to allow comparison and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure what's going on below w/r/t filtered_table = filter_table_results.filtered_table.\n",
    "#is this equivalent to filtered_table, = filter_table_results?\n",
    "\n",
    "#oh, yeah, just read the comment. Is one preferable over the other?\n",
    "\n",
    "\n",
    "#how to approach naming output files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data using the greengenes_reference taxonomy (../output/gg_reference_taxonomy.qza)\n",
      "Removing mitochondia from: <artifact: FeatureTable[Frequency] uuid: 97994d43-af84-4dcd-a4a5-316bcc00a472>\n",
      "Saving results to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_greengenes_reference_mws.qza\n",
      "Saving file summary to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_greengenes_reference_mws.qzv\n",
      "Done with processing greengenes_reference taxonomy annotations!\n",
      "\n",
      "\n",
      "Analyzing data using the greengenes_extended taxonomy (../output/gg_extended_reference_taxonomy.qza)\n",
      "Removing mitochondia from: <artifact: FeatureTable[Frequency] uuid: 97994d43-af84-4dcd-a4a5-316bcc00a472>\n",
      "Saving results to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_greengenes_extended_mws.qza\n",
      "Saving file summary to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_greengenes_extended_mws.qzv\n",
      "Done with processing greengenes_extended taxonomy annotations!\n",
      "\n",
      "\n",
      "Analyzing data using the silva_reference taxonomy (../output/silva_reference_taxonomy.qza)\n",
      "Removing mitochondia from: <artifact: FeatureTable[Frequency] uuid: 97994d43-af84-4dcd-a4a5-316bcc00a472>\n",
      "Saving results to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_silva_reference_mws.qza\n",
      "Saving file summary to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_silva_reference_mws.qzv\n",
      "Done with processing silva_reference taxonomy annotations!\n",
      "\n",
      "\n",
      "Analyzing data using the silva_extended taxonomy (../output/silva_extended_reference_taxonomy.qza)\n",
      "Removing mitochondia from: <artifact: FeatureTable[Frequency] uuid: 97994d43-af84-4dcd-a4a5-316bcc00a472>\n",
      "Saving results to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_silva_extended_mws.qza\n",
      "Saving file summary to: /home/tanya/Work_files/organelle_removal/output/filtered_tables/feature_table_filtered_silva_extended_mws.qzv\n",
      "Done with processing silva_extended taxonomy annotations!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_feature_tables_by_taxonomy = defaultdict(dict)\n",
    "\n",
    "feature_table = Artifact.load(feature_table_path)\n",
    "\n",
    "for label, taxonomy_file in taxonomy_files.items():\n",
    "    print(f\"Analyzing data using the {label} taxonomy ({taxonomy_file})\")\n",
    "    taxonomy = Artifact.load(taxonomy_file)\n",
    "    print(f\"Removing mitochondia from: {feature_table}\")\n",
    "\n",
    "    #Remove organelle sequeces from the feature table\n",
    "    #Note that the Qiime2 API does not return a single object, \n",
    "    #but rather a named Tuple struture with each output, which we save in filter_table_results\n",
    "    filter_table_results = filter_table(feature_table,taxonomy,exclude=\"mitochondria,chloroplast\",mode=\"contains\")\n",
    "    \n",
    "    #Additionally remove any samples not in the metadata\n",
    "    filter_table_results = filter_samples(filter_table_results.filtered_table,metadata=metadata)\n",
    "    filtered_table = filter_table_results.filtered_table\n",
    "    \n",
    "    #Save the filtered feature table to a .qza file in the output directory\n",
    "    output_filename = f\"feature_table_filtered_{label}_mws.qza\"\n",
    "    output_filepath = join(output_dir,output_filename)\n",
    "    print(f\"Saving results to: {output_filepath}\")\n",
    "    filtered_table.save(output_filepath)\n",
    "    \n",
    "    #Output a feature table summary visualization\n",
    "    summary_visualization = summarize(filtered_table,sample_metadata=metadata)\n",
    "    vis = summary_visualization.visualization\n",
    "    output_filename = f\"feature_table_filtered_{label}_mws.qzv\"\n",
    "    output_filepath = join(output_dir,output_filename)\n",
    "    print(f\"Saving file summary to: {output_filepath}\")\n",
    "    \n",
    "    filtered_feature_tables_by_taxonomy[label]=filtered_table\n",
    "    print(f\"Done with processing {label} taxonomy annotations!\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_feature_tables_by_taxonomy = defaultdict(dict)\n",
    "\n",
    "feature_table = Artifact.load(feature_table_path)\n",
    "\n",
    "for reference, classification_taxonomy in vsearch_results.items():\n",
    "    print(f'Analyzing data using the {reference} classification taxonomy')\n",
    "    print(f'Removing mitochondria from: {feature_table_name}')\n",
    "    \n",
    "    #Remove organelle sequeces from the feature table\n",
    "    #Note that the Qiime2 API does not return a single object, \n",
    "    #but rather a named Tuple struture with each output, which we save in filter_table_results\n",
    "    filter_table_results = filter_table(feature_table, classification_taxonomy,\n",
    "                                        exclude = 'mitochondria,chloroplast',\n",
    "                                        mode = 'contains')\n",
    "    #Additionally remove any samples not in the metadata\n",
    "    filter_table_results = filter_samples(filter_table_results.filtered_table,\n",
    "                                          metadata = metadata)\n",
    "    filtered_table = filter_table_results.filtered_table\n",
    "    \n",
    "    #Save the filtered feature table to a .qza file in the output directory\n",
    "    filtered_table_path = join(output_dir, f'feature_table_filtered_{reference}.qza')\n",
    "    print(f'Saving results to: {filtered_table_path}')\n",
    "    filtered_table.save(filtered_table_path)\n",
    "    \n",
    "    #output a feature table summary visualization\n",
    "    summary_visualization = summarize(filtered_table, sample_metadata = metadata)\n",
    "    vis = summary_visualization.visualization\n",
    "    visualization_path = join(output_dir, f'feature_table_filtered_{reference}.qzv')\n",
    "    print(f'Saving file summary to: {visualization_path}')\n",
    "    vis.save(visualization_path)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rarefy tables to an even depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: 44d128c1-3928-45c1-812b-5b4951168af0> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n",
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: 198b8748-a810-412f-b31a-ec9bbdd5def8> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n",
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: c6323e2b-0a78-4e72-939e-08cc227a6085> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n",
      "Rarefying feature table <artifact: FeatureTable[Frequency] uuid: 5bb410e1-75b4-41c1-8887-073b4db4ac1e> to 1000 sequences/sample.\n",
      "Saving results to:{output_filepath}\n"
     ]
    }
   ],
   "source": [
    "#choose a rarefaction depth appropriate for your study.\n",
    "rarefaction_depth = 1000\n",
    "\n",
    "rarefied_feature_tables_by_taxonomy = defaultdict(dict)\n",
    "\n",
    "for label,filtered_feature_tables in filtered_feature_tables_by_taxonomy.items():\n",
    "    print(f\"Rarefying feature table {filtered_feature_tables} to {rarefaction_depth} sequences/sample.\")\n",
    "    rarefy_results = rarefy(table=filtered_feature_tables,sampling_depth=rarefaction_depth)\n",
    "    #get the rarefied table out of the NamedTuple of results\n",
    "    rarefied_filtered_table = rarefy_results.rarefied_table\n",
    "        \n",
    "    #save the resulting feature table\n",
    "    output_filename = f\"feature_table_{label}_{rarefaction_depth}.qza\"\n",
    "    output_filepath = join(output_dir,output_filename)\n",
    "    print(\"Saving results to:{output_filepath}\")\n",
    "    rarefied_filtered_table.save(output_filepath)\n",
    "        \n",
    "    #store the rarefied tables in a dict so they don't need to relode them\n",
    "    rarefied_feature_tables_by_taxonomy[label] = rarefied_filtered_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
